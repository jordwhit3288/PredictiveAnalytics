---
title: "Assignment1"
author: "Jordan Whitaker"
date: "5/25/2020"
output: word_document
---

```{r}
library(tidyverse)
library(MASS)
library(caret)
```

```{r}
bike = read_csv("hour.csv")
```

```{r}
bike = bike %>% mutate(season = as_factor(as.character(season))) %>%
mutate(season = fct_recode(season,
"Spring" = "1",
"Summer" = "2",
"Fall" = "3",
"Winter" = "4"))

#head(bike) #key reference to ensure code snippets below were successful 

bike = bike %>% mutate(yr = as_factor(as.character(yr))) %>%
mutate(yr = fct_recode(yr))


bike = bike %>% mutate(mnth = as_factor(as.character(mnth))) %>%
mutate(mnth = fct_recode(mnth))

bike = bike %>% mutate(hr = as_factor(as.character(hr))) %>%
mutate(hr = fct_recode(hr))


bike = bike %>% mutate(workingday = as_factor(as.character(workingday))) %>%
mutate(workingday = fct_recode(workingday,
"NotWorkingDay" = "0",
"WorkingDay" = "1"))


bike = bike %>% mutate(weathersit = as_factor(as.character(weathersit))) %>%
mutate(weathersit = fct_recode(weathersit,
"NoPrecip" = "1",
"Misty" = "2",
"LightPrecip" = "3",
"HeavyPrecip" = "4"))


bike = bike %>% mutate(weekday = as_factor(as.character(weekday))) %>%
mutate(weekday = fct_recode(weekday,
"Monday" = "1",
"Tuesday" = "2",
"Wednesday" = "3",
"Thursday" = "4",
"Friday" = "5",
"Saturday" = "6",
"Sunday"= "0"))

head(bike)
```


```{r}

set.seed(1234)
train.rows = createDataPartition(y= bike$count, p=0.7, list= FALSE)

train = bike[train.rows,]
test = bike[-train.rows,]

```
Task 2: There are 12167 rows in the training dataset and there are 5212 rows in the testing dataset.


```{r}
train_mod = lm(count ~ season + mnth + hr + weekday + temp + weathersit, bike)
summary(train_mod)

```

Task 3: The model above is pretty good with an adjusted R-Squared value of .62, most variables are significant.


```{r}
predict_train = predict(train_mod, newdata = train)

head(predict_train)
View(predict_train)
summary(predict_train)
hist(predict_train)
```


The model has some interesting predictions.  There are roughly 1000 combined instances where there is a negative count prediction.  The distribution is mostly normal. 


```{r}
predict_test = predict(train_mod, newdata = test)

head(predict_test)
View(predict_test)
summary(predict_test)
hist(predict_test)


```

Task 5: The testing data has a similar distrubtion compared to the training data predictions.  There are still some negative predictions but the overall trend of the distrubtion is closely related. 



```{r}
SSE = sum((test$count - predict_test)^2) #sum of squared errors

SST = sum((test$count - mean(test$count))^2) #sum of squared residuals from a "naive" model

r_squared = 1 - SSE/SST
r_squared # .6309
````

Task 6: Manually calculating R-Squared value
The manually calculated R-squared value on the testing data is .6309 compared to .6227 on the training data model.  These closely related r-squared values point to good fitting model. It is slightly above, so there may be some overfitting, but not a tremendous amount.


Task 7: K-fold cross validation is a technique that splits the data into partitions, typically k is either 3, 5, or 10.  Using these partitions, the model will evaluate k times and upon each iteration will hold out 1 fold.  This fold will rotate between partitions until each partition has been evaluated as a part of the training set.  Compared to the training/testing split technique which just takes a majority of the data at a random seed and designates it as training data and holds out a minority percentage that will be used to evaluate the training data model.











